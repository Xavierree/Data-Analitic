{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c18cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harmly/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instalasi & Impor Awal Selesai.\n"
     ]
    }
   ],
   "source": [
    "# Instalasi dependensi dari requirements.txt\n",
    "#!pip install torch==2.0.1 torchvision==0.15.2 tqdm==4.65.0 pandas==1.5.3 matplotlib==3.7.1 scikit-learn==1.2.2 torchmetrics==0.11.4 albumentations==1.3.0 opencv-python-headless==4.8.0.76\n",
    "\n",
    "# Impor pustaka dasar yang akan digunakan di beberapa sel\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "from xml.etree import ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(\"✅ Instalasi & Impor Awal Selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d338e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses data latih...\n",
      "⚠️ Peringatan: Anotasi 'immature-107_jpg.rf.e6c1dbeaacc81094cc0c733ccf439233(1).xml' tidak ditemukan, gambar 'immature-107_jpg.rf.e6c1dbeaacc81094cc0c733ccf439233(1).jpg' dilewati.\n",
      "\n",
      "Memproses data validasi...\n",
      "\n",
      "--- Ringkasan ---\n",
      "Total gambar yang dipertimbangkan untuk latih: 1296\n",
      "Total gambar yang berhasil disalin (dengan anotasi): 1295\n",
      "\n",
      "Total gambar yang dipertimbangkan untuk validasi: 325\n",
      "Total gambar yang berhasil disalin (dengan anotasi): 325\n",
      "\n",
      "✅ Pemisahan data selesai. Data disimpan di: ../data/Katarak2_Split\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: split_data.py (Diperbaiki dengan Pengecekan File) ---\n",
    "\n",
    "# Konfigurasi \n",
    "DATASET_DIR = \"../data/Gabungan\"\n",
    "IMAGE_DIR_NAME = \"Image\"\n",
    "ANNOT_DIR_NAME = \"Annotation\"\n",
    "OUTPUT_DIR = \"../data/Katarak2_Split\"\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Sisa kode akan berjalan otomatis berdasarkan konfigurasi di atas\n",
    "IMAGE_DIR = os.path.join(DATASET_DIR, IMAGE_DIR_NAME)\n",
    "ANNOT_DIR = os.path.join(DATASET_DIR, ANNOT_DIR_NAME)\n",
    "\n",
    "if os.path.exists(IMAGE_DIR) and os.path.exists(ANNOT_DIR):\n",
    "    TRAIN_IMG_DIR = os.path.join(OUTPUT_DIR, \"train\", \"Image\")\n",
    "    TRAIN_ANNOT_DIR = os.path.join(OUTPUT_DIR, \"train\", \"Annotation\")\n",
    "    VAL_IMG_DIR = os.path.join(OUTPUT_DIR, \"val\", \"Image\")\n",
    "    VAL_ANNOT_DIR = os.path.join(OUTPUT_DIR, \"val\", \"Annotation\")\n",
    "    \n",
    "    os.makedirs(TRAIN_IMG_DIR, exist_ok=True)\n",
    "    os.makedirs(TRAIN_ANNOT_DIR, exist_ok=True)\n",
    "    os.makedirs(VAL_IMG_DIR, exist_ok=True)\n",
    "    os.makedirs(VAL_ANNOT_DIR, exist_ok=True)\n",
    "    \n",
    "    image_filenames = [f for f in os.listdir(IMAGE_DIR) if f.endswith((\".jpg\", \".png\"))]\n",
    "    train_images, val_images = train_test_split(image_filenames, train_size=TRAIN_RATIO, random_state=42)\n",
    "    \n",
    "    # --- FUNGSI copy_files YANG DIPERBAIKI ---\n",
    "    def copy_files(file_list, src_img, src_annot, dst_img, dst_annot):\n",
    "        copied_count = 0\n",
    "        for filename in file_list:\n",
    "            annot_filename = os.path.splitext(filename)[0] + \".xml\"\n",
    "            src_annot_path = os.path.join(src_annot, annot_filename)\n",
    "            \n",
    "            # PERBAIKAN: Cek apakah file anotasi ada sebelum menyalin\n",
    "            if os.path.exists(src_annot_path):\n",
    "                # Salin gambar\n",
    "                shutil.copy(os.path.join(src_img, filename), os.path.join(dst_img, filename))\n",
    "                # Salin anotasi\n",
    "                shutil.copy(src_annot_path, os.path.join(dst_annot, annot_filename))\n",
    "                copied_count += 1\n",
    "            else:\n",
    "                # Beri peringatan jika anotasi tidak ditemukan\n",
    "                print(f\"⚠️ Peringatan: Anotasi '{annot_filename}' tidak ditemukan, gambar '{filename}' dilewati.\")\n",
    "        return copied_count\n",
    "\n",
    "    print(\"Memproses data latih...\")\n",
    "    num_train = copy_files(train_images, IMAGE_DIR, ANNOT_DIR, TRAIN_IMG_DIR, TRAIN_ANNOT_DIR)\n",
    "    \n",
    "    print(\"\\nMemproses data validasi...\")\n",
    "    num_val = copy_files(val_images, IMAGE_DIR, ANNOT_DIR, VAL_IMG_DIR, VAL_ANNOT_DIR)\n",
    "    \n",
    "    print(\"\\n--- Ringkasan ---\")\n",
    "    print(f\"Total gambar yang dipertimbangkan untuk latih: {len(train_images)}\")\n",
    "    print(f\"Total gambar yang berhasil disalin (dengan anotasi): {num_train}\")\n",
    "    print(f\"\\nTotal gambar yang dipertimbangkan untuk validasi: {len(val_images)}\")\n",
    "    print(f\"Total gambar yang berhasil disalin (dengan anotasi): {num_val}\")\n",
    "    print(f\"\\n✅ Pemisahan data selesai. Data disimpan di: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"❌ Direktori data sumber tidak ditemukan di '{IMAGE_DIR}' atau '{ANNOT_DIR}'. Periksa kembali path Anda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67eaee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfigurasi dimuat. Device: cuda, Output Dir: outputs\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: config.py ---\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "RESIZE_TO = 640\n",
    "NUM_EPOCHS = 10\n",
    "NUM_WORKERS = 4 # Sesuaikan dengan CPU Anda\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# PENTING: Sesuaikan path ini dengan struktur folder Anda\n",
    "TRAIN_IMG = '../data/train/Image'\n",
    "TRAIN_ANNOT = '../data/train/Annotation'\n",
    "VALID_IMG = '../data/Katarak2_Split/val/Image'\n",
    "VALID_ANNOT = '../data/Katarak2_Split/val/Annotation'\n",
    "\n",
    "CLASSES = ['__background__', 'Immature', 'Mature', 'Normal']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "OUT_DIR = 'outputs'\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Konfigurasi dimuat. Device: {DEVICE}, Output Dir: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1d85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fungsi utilitas ('custom_utils.py') berhasil didefinisikan.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: custom_utils.py ---\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self): self.reset()\n",
    "    def send(self, value): self.current_total += value; self.iterations += 1\n",
    "    @property\n",
    "    def value(self): return self.current_total / self.iterations if self.iterations > 0 else 0\n",
    "    def reset(self): self.current_total = 0.0; self.iterations = 0.0\n",
    "\n",
    "class SaveBestModel:\n",
    "    def __init__(self, best_valid_map=0.0): self.best_valid_map = best_valid_map\n",
    "    def __call__(self, model, current_valid_map, epoch, out_dir):\n",
    "        if current_valid_map > self.best_valid_map:\n",
    "            self.best_valid_map = current_valid_map\n",
    "            file_name = f\"{out_dir}/best_model_mAP_{current_valid_map:.4f}.pth\"\n",
    "            print(f\"\\nValid mAP meningkat. Menyimpan model ke {file_name}\")\n",
    "            torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict()}, file_name)\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.3), A.Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0)], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([ToTensorV2(p=1.0)], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def save_loss_plot(out_dir, train_loss_list):\n",
    "    plt.figure(figsize=(10, 7)); plt.plot(train_loss_list, label='Train Loss')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title(\"Training Loss Plot\"); plt.legend()\n",
    "    plt.savefig(f\"{out_dir}/train_loss.png\"); plt.close()\n",
    "\n",
    "def save_mAP(out_dir, map_50_list, map_95_list):\n",
    "    plt.figure(figsize=(10, 7)); plt.plot(map_50_list, label='mAP@0.5', color='orange')\n",
    "    plt.plot(map_95_list, label='mAP@0.5:0.95', color='red'); plt.xlabel('Epochs')\n",
    "    plt.ylabel('mAP'); plt.title('mAP Over Epochs'); plt.legend()\n",
    "    plt.savefig(f\"{out_dir}/map_plot.png\"); plt.close()\n",
    "\n",
    "print(\"✅ Fungsi utilitas ('custom_utils.py') berhasil didefinisikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c189add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kelas Dataset ('datasets.py') berhasil didefinisikan.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: datasets.py ---\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path, annot_path, width, height, classes, transforms=None):\n",
    "        self.img_path, self.annot_path = img_path, annot_path\n",
    "        self.width, self.height = width, height\n",
    "        self.classes, self.transforms = classes, transforms\n",
    "        self.all_images = sorted(glob.glob(os.path.join(img_path, \"*.jpg\")))\n",
    "    def __len__(self): return len(self.all_images)\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = os.path.basename(self.all_images[idx])\n",
    "        annot_path = os.path.join(self.annot_path, image_name.replace('.jpg', '.xml'))\n",
    "        image = cv2.imread(self.all_images[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        \n",
    "        tree = ET.parse(annot_path)\n",
    "        root = tree.getroot()\n",
    "        boxes, labels = [], []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            label = self.classes.index(obj.find(\"name\").text)\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            xmin, ymin, xmax, ymax = map(int, [bndbox.find(tag).text for tag in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(label)\n",
    "        \n",
    "        # Buat dictionary target\n",
    "        target = {}\n",
    "\n",
    "        # Lakukan transformasi jika ada\n",
    "        if self.transforms:\n",
    "            # Konversi ke NumPy array SEBELUM augmentasi (INI PERBAIKANNYA)\n",
    "            sample = self.transforms(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                labels=labels\n",
    "            )\n",
    "            image = sample['image']\n",
    "            # Perbarui boxes dan labels DARI HASIL augmentasi (INI PERBAIKANNYA)\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32) if len(sample['bboxes']) > 0 else torch.empty((0, 4))\n",
    "            target['labels'] = torch.tensor(sample['labels'], dtype=torch.int64) if len(sample['labels']) > 0 else torch.empty(0, dtype=torch.int64)\n",
    "        else:\n",
    "            # Jika tidak ada transformasi, konversi langsung ke tensor\n",
    "            target['boxes'] = torch.tensor(boxes, dtype=torch.float32)\n",
    "            target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch): return tuple(zip(*batch))\n",
    "\n",
    "print(\"✅ Kelas Dataset ('datasets.py') berhasil didefinisikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869dcb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data di validation dataset: 325\n",
      "✅ Berhasil membuat validation loader. Seharusnya masalah path sudah benar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harmly/.local/lib/python3.10/site-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "# Jalankan ini di sel baru untuk diagnosis\n",
    "try:\n",
    "    debug_valid_dataset = CustomDataset(VALID_IMG, VALID_ANNOT, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "    debug_valid_loader = DataLoader(debug_valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Jumlah data di validation dataset: {len(debug_valid_dataset)}\")\n",
    "    if len(debug_valid_dataset) > 0:\n",
    "        print(\"✅ Berhasil membuat validation loader. Seharusnya masalah path sudah benar.\")\n",
    "    else:\n",
    "        print(\"🔴 Peringatan: Validation dataset kosong! Periksa isi folder validasi Anda.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Terjadi error saat membuat validation loader: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b320958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fungsi pembuatan model ('model.py') berhasil didefinisikan.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: model.py ---\n",
    "\n",
    "def create_model(num_classes, min_size=640, max_size=640):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.transform.min_size, model.transform.max_size = (min_size,), max_size\n",
    "    return model\n",
    "\n",
    "print(\"✅ Fungsi pembuatan model ('model.py') berhasil didefinisikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bf84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data latih: 1500 | Data validasi: 325\n",
      "\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0804: 100%|██████████| 188/188 [02:16<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Pelatihan: 0.1983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 41/41 [00:12<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@0.5: 0.7205, mAP@0.5:0.95: 0.5157, Precision: 0.4428, F1: 0.4330\n",
      "\n",
      "Valid mAP meningkat. Menyimpan model ke outputs/best_model_mAP_0.5157.pth\n",
      "\n",
      "--- Epoch 2/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0546: 100%|██████████| 188/188 [02:12<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Pelatihan: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 41/41 [00:12<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@0.5: 0.8981, mAP@0.5:0.95: 0.6821, Precision: 0.8926, F1: 0.8777\n",
      "\n",
      "Valid mAP meningkat. Menyimpan model ke outputs/best_model_mAP_0.6821.pth\n",
      "\n",
      "--- Epoch 3/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0623: 100%|██████████| 188/188 [02:12<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Pelatihan: 0.0547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 41/41 [00:12<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@0.5: 0.9373, mAP@0.5:0.95: 0.6881, Precision: 0.7643, F1: 0.7146\n",
      "\n",
      "Valid mAP meningkat. Menyimpan model ke outputs/best_model_mAP_0.6881.pth\n",
      "\n",
      "--- Epoch 4/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0663: 100%|██████████| 188/188 [02:12<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Pelatihan: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 41/41 [00:15<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@0.5: 0.8529, mAP@0.5:0.95: 0.6548, Precision: 0.6135, F1: 0.4093\n",
      "\n",
      "--- Epoch 5/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0421:  98%|█████████▊| 185/188 [02:07<00:02,  1.28it/s] "
     ]
    }
   ],
   "source": [
    "# --- Cell 7: train.py ---\n",
    "\n",
    "# Inisialisasi datasets dan loaders\n",
    "train_dataset = CustomDataset(TRAIN_IMG, TRAIN_ANNOT, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\n",
    "valid_dataset = CustomDataset(VALID_IMG, VALID_ANNOT, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "print(f\"Data latih: {len(train_dataset)} | Data validasi: {len(valid_dataset)}\\n\")\n",
    "\n",
    "# Inisialisasi model dan optimizer\n",
    "model = create_model(NUM_CLASSES, min_size=RESIZE_TO, max_size=RESIZE_TO).to(DEVICE)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.99, weight_decay=0.0005)\n",
    "\n",
    "# Inisialisasi pelacak metrik\n",
    "train_loss_hist = Averager()\n",
    "map_metric = MeanAveragePrecision()\n",
    "save_best_model = SaveBestModel()\n",
    "metrics_data, map_50_list, map_95_list = [], [], []\n",
    "\n",
    "# Loop Pelatihan\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "    train_loss_hist.reset(); model.train()\n",
    "    prog_bar = tqdm(train_loader, total=len(train_loader))\n",
    "    \n",
    "    # Fase Training\n",
    "    for images, targets in prog_bar:\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward(); optimizer.step()\n",
    "        train_loss_hist.send(losses.item())\n",
    "        prog_bar.set_description(f\"Loss: {losses.item():.4f}\")\n",
    "    print(f\"Loss Pelatihan: {train_loss_hist.value:.4f}\")\n",
    "\n",
    "    # Fase Validasi\n",
    "    model.eval(); map_metric.reset()\n",
    "    all_true_labels, all_pred_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(valid_loader, desc=\"Validating\"):\n",
    "            images = [img.to(DEVICE) for img in images]; outputs = model(images)\n",
    "            for i in range(len(images)):\n",
    "                true_labels, true_boxes = targets[i][\"labels\"].cpu(), targets[i][\"boxes\"].cpu()\n",
    "                pred_scores, pred_labels, pred_boxes = outputs[i][\"scores\"].cpu(), outputs[i][\"labels\"].cpu(), outputs[i][\"boxes\"].cpu()\n",
    "                mask = pred_scores > 0.5\n",
    "                map_metric.update([{\"boxes\": pred_boxes[mask], \"scores\": pred_scores[mask], \"labels\": pred_labels[mask]}], \n",
    "                                  [{\"boxes\": true_boxes, \"labels\": true_labels}])\n",
    "                all_true_labels.extend(true_labels.numpy())\n",
    "                all_pred_labels.extend(pred_labels[mask].numpy())\n",
    "\n",
    "    # Kalkulasi Metrik\n",
    "    map_result = map_metric.compute()\n",
    "    min_len = min(len(all_true_labels), len(all_pred_labels))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_true_labels[:min_len], all_pred_labels[:min_len], average=\"weighted\", zero_division=0)\n",
    "    print(f\"mAP@0.5: {map_result['map_50']:.4f}, mAP@0.5:0.95: {map_result['map']:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Simpan hasil\n",
    "    map_50_list.append(map_result['map_50'].item()); map_95_list.append(map_result['map'].item())\n",
    "    metrics_data.append({\"Epoch\": epoch+1, \"Train Loss\": train_loss_hist.value, \"mAP@0.5\": map_result['map_50'].item(), \"mAP@0.5:0.95\": map_result['map'].item(), \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1})\n",
    "    train_losses = [e[\"Train Loss\"] for e in metrics_data]\n",
    "    save_loss_plot(OUT_DIR, train_losses); save_mAP(OUT_DIR, map_50_list, map_95_list)\n",
    "    save_best_model(model, map_result['map'].item(), epoch, OUT_DIR)\n",
    "\n",
    "# Simpan metrik ke Excel\n",
    "pd.DataFrame(metrics_data).to_excel(f\"{OUT_DIR}/all_metrics.xlsx\", index=False)\n",
    "print(\"\\n✅ Pelatihan Selesai!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
